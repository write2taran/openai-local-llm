# openai-local-llm
Local OpenAI-compatible API gateway powered by Ollama, enabling drop-in replacement of cloud LLMs with on-device inference.
